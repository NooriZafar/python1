## performing pyspark sql


simpleData = [("James","Sales","NY",90000,34,10000),
    ("Michael","Sales","NY",86000,56,20000),
    ("Robert","Sales","CA",81000,30,23000),
    ("Maria","Finance","CA",90000,24,23000),
    ("Raman","Finance","CA",99000,40,24000),
    ("Scott","Finance","NY",83000,36,19000),
    ("Jen","Finance","NY",79000,53,15000),
    ("Jeff","Marketing","CA",80000,25,18000),
    ("Kumar","Marketing","NY",91000,50,21000)
  ]

schema = ["employee_name","department","state","salary","age","bonus"]
df = spark.createDataFrame(data=simpleData, schema = schema)
df.printSchema()
df.show(truncate=False)

output:

root
 |-- employee_name: string (nullable = true)
 |-- department: string (nullable = true)
 |-- state: string (nullable = true)
 |-- salary: long (nullable = true)
 |-- age: long (nullable = true)
 |-- bonus: long (nullable = true)

+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|James        |Sales     |NY   |90000 |34 |10000|
|Michael      |Sales     |NY   |86000 |56 |20000|
|Robert       |Sales     |CA   |81000 |30 |23000|
|Maria        |Finance   |CA   |90000 |24 |23000|
|Raman        |Finance   |CA   |99000 |40 |24000|
|Scott        |Finance   |NY   |83000 |36 |19000|
|Jen          |Finance   |NY   |79000 |53 |15000|
|Jeff         |Marketing |CA   |80000 |25 |18000|
|Kumar        |Marketing |NY   |91000 |50 |21000|
+-------------+----------+-----+------+---+-----+


## creating view to the above table

df.createOrReplaceTempView("df")

## To retrieve the data

spark.sql("select * from df").show()

output:

+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        James|     Sales|   NY| 90000| 34|10000|
|      Michael|     Sales|   NY| 86000| 56|20000|
|       Robert|     Sales|   CA| 81000| 30|23000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|        Raman|   Finance|   CA| 99000| 40|24000|
|        Scott|   Finance|   NY| 83000| 36|19000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
+-------------+----------+-----+------+---+-----+


## Query for outputting sorted data using "order by"

spark.sql("select * from df order by employee_name asc").show()

output:

+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        James|     Sales|   NY| 90000| 34|10000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|      Michael|     Sales|   NY| 86000| 56|20000|
|        Raman|   Finance|   CA| 99000| 40|24000|
|       Robert|     Sales|   CA| 81000| 30|23000|
|        Scott|   Finance|   NY| 83000| 36|19000|
+-------------+----------+-----+------+---+-----+



## group by

spark.sql("select employee_name,department from df group by department,employee_name ").show()

output:

+-------------+----------+
|employee_name|department|
+-------------+----------+
|        James|     Sales|
|      Michael|     Sales|
|       Robert|     Sales|
|        Maria|   Finance|
|        Raman|   Finance|
|        Scott|   Finance|
|          Jen|   Finance|
|         Jeff| Marketing|
|        Kumar| Marketing|
+-------------+----------+


## performing "count" on table

spark.sql("select count(employee_name)as count from df").show()

output:

+-----+
|count|
+-----+
|    9|
+-----+

## count along with condition


spark.sql("select count(employee_name) from df  where salary>80000").show()

output:

+--------------------+
|count(employee_name)|
+--------------------+
|                   7|
+--------------------+

## collecting 'avg' of salary

spark.sql("select avg(salary) from df where department='Sales'").show()

output:

+-----------------+
|      avg(salary)|
+-----------------+
|85666.66666666667|
+-----------------+

# sum of salary 

spark.sql("select sum(salary) from df where department='Sales'").show()

output:

+-----------+
|sum(salary)|
+-----------+
|     257000|
+-----------+

## query for creating a temp view

spark.sql("create temp view df1 as select df.employee_name, df.salary from df").show()

#query for showing details of view

spark.sql("select * from df1").show()

output:

+-------------+------+
|employee_name|salary|
+-------------+------+
|        James| 90000|
|      Michael| 86000|
|       Robert| 81000|
|        Maria| 90000|
|        Raman| 99000|
|        Scott| 83000|
|          Jen| 79000|
|         Jeff| 80000|
|        Kumar| 91000|
+-------------+------+

## query for dropping a view

spark.sql("drop view df1").show()

output:

++
||
++
++

## query to display all the views created

spark.sql("show views").show()

output:

+---------+---------+-----------+
|namespace| viewName|isTemporary|
+---------+---------+-----------+
|         |       df|       true|
|         |      df1|       true|
|         | df_view9|       true|
|         |df_view99|       true|
+---------+---------+-----------+

## displaying a column of unique values

spark=SparkSession.builder.appName('duplicate').getOrCreate()
emp=[("noori",25),
     ("sabeeha",22),
     ("noori",25)
    ]
empc=["names","age"]
empDF=spark.createDataFrame(emp,empc)
empDF.show()

output:

+-------+---+
|  names|age|
+-------+---+
|  noori| 25|
|sabeeha| 22|
|  noori| 25|
+-------+---+

empDF.createOrReplaceTempView("empDF")
spark.sql("select DISTINCT names from empDF").show()

output:

+-------+
|  names|
+-------+
|  noori|
|sabeeha|
+-------+


##searching for tables with wildcards

spark.sql("select * from df where employee_name like 'J%'").show()

output:

+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        James|     Sales|   NY| 90000| 34|10000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
+-------------+----------+-----+------+---+-----+

##finding the intersection of two tables


empDF.intersect(empDF1).show()

output:

+-------+---+
|  names|age|
+-------+---+
|   ammi| 25|
|  noori| 25|
|sabeeha| 22|
+-------+---+


spark.sql("select names from empDF join empDF1 on empDF.age==empDF1.age intersect select names from empDF full join empDF1 on empDF.age==empDF1.age ").show()


output:

+-------+
|  names|
+-------+
|  noori|
|sabeeha|
+-------+


swapping the values of two columns in a table

df.select(col("salary").alias("bonus"),col("bonus").alias("salary")).show()

output:

+-----+------+
|bonus|salary|
+-----+------+
|90000| 10000|
|86000| 20000|
|81000| 23000|
|90000| 23000|
|99000| 24000|
|83000| 19000|
|79000| 15000|
|80000| 18000|
|91000| 21000|
+-----+------+






