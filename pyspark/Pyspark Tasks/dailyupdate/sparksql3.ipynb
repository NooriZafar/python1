{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "831f9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cae1e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b763767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('practice').getOrCreate()\n",
    "df=spark.sql(\"select 'spark' as hello\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd857ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n",
      "|player_id|device_id|event_date|games_played|\n",
      "+---------+---------+----------+------------+\n",
      "|        1|        2|2016-03-01|           5|\n",
      "|        1|        2|2016-05-02|           2|\n",
      "|        2|        3|2017-06-25|           1|\n",
      "|        3|        1|2016-03-02|           0|\n",
      "|        3|        4|2018-07-03|           5|\n",
      "+---------+---------+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "activity=[(1,2,'2016-03-01',5),(1,2,'2016-05-02',2),(2,3,'2017-06-25',1),\n",
    "            (3,1,'2016-03-02',0),(3,4,'2018-07-03',5)]\n",
    "activitycolumns=[\"player_id\",\"device_id\",\"event_date\",\"games_played\"]\n",
    "activitydf=spark.createDataFrame(activity,activitycolumns)\n",
    "activitydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdab784",
   "metadata": {},
   "outputs": [],
   "source": [
    "activitydf.createOrReplaceTempView(\"activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74eaed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|player_id|first_login|\n",
      "+---------+-----------+\n",
      "|        1| 2016-03-01|\n",
      "|        2| 2017-06-25|\n",
      "|        3| 2016-03-02|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT player_id, MIN(event_date) AS first_login\n",
    "FROM Activity  GROUP BY player_id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6ecf3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "|order_number|customer_number|\n",
      "+------------+---------------+\n",
      "|           1|              1|\n",
      "|           2|              2|\n",
      "|           3|              3|\n",
      "|           4|              3|\n",
      "|           5|              2|\n",
      "|           6|              3|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders=[(1,1),(2,2),(3,3),(4,3),(5,2),(6,3)]\n",
    "ordercolumns=[\"order_number\",\"customer_number\"]\n",
    "orderdf=spark.createDataFrame(orders,ordercolumns)\n",
    "orderdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45783f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "orderdf.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06de2eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|customer_number|\n",
      "+---------------+\n",
      "|              3|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT customer_number\n",
    "FROM\n",
    "    orders\n",
    "GROUP BY customer_number order by count(*) desc limit 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef10f4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|student|      class|\n",
      "+-------+-----------+\n",
      "|      A|Mathematics|\n",
      "|      B|    English|\n",
      "|      C|Mathematics|\n",
      "|      D|    Biology|\n",
      "|      E|Mathematics|\n",
      "|      F|   Computer|\n",
      "|      G|Mathematics|\n",
      "|      H|Mathematics|\n",
      "|      I|Mathematics|\n",
      "|      J|    Biology|\n",
      "+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course=[('A', 'Mathematics'),('B', 'English'),('C', 'Mathematics'),\n",
    "('D', 'Biology'),('E', 'Mathematics'),('F', 'Computer'),('G', 'Mathematics'),('H', 'Mathematics'),\n",
    "('I', 'Mathematics'),('J','Biology')]\n",
    "coursecolumns=[\"student\",\"class\"]\n",
    "coursedf=spark.createDataFrame(course,coursecolumns)\n",
    "coursedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d57baf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "coursedf.createOrReplaceTempView(\"courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cb89ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|      class|count(1)|\n",
      "+-----------+--------+\n",
      "|Mathematics|       6|\n",
      "|    Biology|       2|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select class,count(*) from courses group by class having count(distinct student)>=2\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b0065ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|parent_id|\n",
      "+---+---------+\n",
      "|  1|     null|\n",
      "|  2|        1|\n",
      "|  3|        1|\n",
      "|  4|        2|\n",
      "|  5|        2|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tree=[('1', None),('2', '1'),('3', '1'),('4', '2'),('5', '2')]\n",
    "treecolumns=[\"id\",\"parent_id\"]\n",
    "treedf=spark.createDataFrame(tree,treecolumns)\n",
    "treedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68993656",
   "metadata": {},
   "outputs": [],
   "source": [
    "treedf.createOrReplaceTempView(\"Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6f9dd19",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o370.showString.\n: java.lang.IllegalStateException: more than one row returned by a subquery used as an expression:\nSubquery subquery#649, [id=#2019]\n+- AdaptiveSparkPlan isFinalPlan=true\n   +- == Final Plan ==\n      *(1) Project [parent_id#654]\n      +- *(1) Scan ExistingRDD[id#653,parent_id#654]\n   +- == Initial Plan ==\n      Project [parent_id#654]\n      +- Scan ExistingRDD[id#653,parent_id#654]\n\r\n\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:84)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:262)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:261)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:261)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:231)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:373)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7536\\2520130879.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m spark.sql(\"\"\"SELECT\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mid\u001b[0m \u001b[0mAS\u001b[0m \u001b[0mId\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mCASE\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mWHEN\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSELECT\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0mtree\u001b[0m \u001b[0mt\u001b[0m \u001b[0mWHERE\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_id\u001b[0m \u001b[0mIS\u001b[0m \u001b[0mNULL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mTHEN\u001b[0m \u001b[1;34m'Root'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o370.showString.\n: java.lang.IllegalStateException: more than one row returned by a subquery used as an expression:\nSubquery subquery#649, [id=#2019]\n+- AdaptiveSparkPlan isFinalPlan=true\n   +- == Final Plan ==\n      *(1) Project [parent_id#654]\n      +- *(1) Scan ExistingRDD[id#653,parent_id#654]\n   +- == Initial Plan ==\n      Project [parent_id#654]\n      +- Scan ExistingRDD[id#653,parent_id#654]\n\r\n\tat org.apache.spark.sql.execution.ScalarSubquery.updateResult(subquery.scala:84)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1(SparkPlan.scala:262)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$waitForSubqueries$1$adapted(SparkPlan.scala:261)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.sql.execution.SparkPlan.waitForSubqueries(SparkPlan.scala:261)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:231)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)\r\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)\r\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:42)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:660)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:723)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:204)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:373)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT\n",
    "    id AS Id,    \n",
    "    CASE 1=1\n",
    "        WHEN tree.id = (SELECT t.id FROM tree t WHERE t.parent_id IS NULL)         \n",
    "        THEN 'Root'        \n",
    "        WHEN tree.id = (SELECT t.parent_id FROM tree t)        \n",
    "        THEN 'Inner'     \n",
    "        ELSE 'Leaf'   \n",
    "        END AS Type\n",
    "FROM\n",
    "    tree\n",
    "ORDER BY Id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6e087e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+\n",
      "|actor_id|director_id|timestamp|\n",
      "+--------+-----------+---------+\n",
      "|       1|          1|        0|\n",
      "|       1|          1|        1|\n",
      "|       1|          1|        2|\n",
      "|       1|          2|        3|\n",
      "|       1|          2|        4|\n",
      "|       2|          1|        5|\n",
      "|       2|          1|        6|\n",
      "+--------+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actor=[('1', '1', '0'),('1', '1', '1'),('1', '1', '2'),('1', '2', '3'),\n",
    "('1', '2', '4'),('2', '1', '5'),('2', '1', '6')]\n",
    "actorcolumns=[\"actor_id\",\"director_id\",\"timestamp\"]\n",
    "actordf=spark.createDataFrame(actor,actorcolumns)\n",
    "actordf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "32dd7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "actordf.createOrReplaceTempView(\"actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a45e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|actor_id|director_id|\n",
      "+--------+-----------+\n",
      "|       1|          1|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select actor_id,director_id from\n",
    "(select actor_id,director_id,COUNT(timestamp) AS nums\n",
    "FROM Actor\n",
    "GROUP BY actor_id, director_id\n",
    "ORDER BY actor_id, director_id) tmp\n",
    "WHERE nums >= 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7b4dbba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n",
      "| id|  name|profession|\n",
      "+---+------+----------+\n",
      "|  1| Noori|    Doctor|\n",
      "|  2|Nayeem|     Model|\n",
      "|  3| Azgar|  business|\n",
      "|  4|Suhail|     Actor|\n",
      "|  5|  Saif|     Actor|\n",
      "+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "identity=[(1,'Noori','Doctor'),(2,'Nayeem','Model'),(3,'Azgar','business'),\n",
    "\t\t\t\t\t\t\t(4,'Suhail','Actor'),(5,'Saif','Actor')]\n",
    "identitycolumns=[\"id\",\"name\",\"profession\"]\n",
    "identitydf=spark.createDataFrame(identity,identitycolumns)\n",
    "identitydf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3732ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "identitydf.createOrReplaceTempView(\"identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49cc1b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| fullname|\n",
      "+---------+\n",
      "| Noori(D)|\n",
      "|Nayeem(M)|\n",
      "| Azgar(b)|\n",
      "|Suhail(A)|\n",
      "|  Saif(A)|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select concat(name, '(', substr(profession,1,1), ')') as fullname from identity\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bff1d0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|user_id|start_date|  end_date|\n",
      "+-------+----------+----------+\n",
      "|      1|2022-01-01|2022-01-31|\n",
      "|      2|2022-01-16|2022-01-26|\n",
      "|      3|2022-01-28|2022-02-06|\n",
      "|      4|2022-02-16|2022-02-26|\n",
      "+-------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subscription=[(1,'2022-01-01','2022-01-31'),(2,'2022-01-16','2022-01-26'),\n",
    "\t\t\t\t\t\t\t\t(3,'2022-01-28','2022-02-06'),(4,'2022-02-16','2022-02-26')]\n",
    "subscriptioncolumns=[\"user_id\",\"start_date\",\"end_date\"]\n",
    "subscriptiondf=spark.createDataFrame(subscription,subscriptioncolumns)\n",
    "subscriptiondf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dbbb7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcriptiondf.createOrReplaceTempView(\"subscription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c9cbb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|overlap|\n",
      "+-------+-------+\n",
      "|      1|   true|\n",
      "|      2|   true|\n",
      "|      3|   true|\n",
      "|      4|  false|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select distinct s1.user_id,(\n",
    "  case when s2.user_id is not null then 'true' else 'false' end) as overlap\n",
    "  from subscription as s1 left join subscription s2\n",
    "  on s1.user_id != s2.user_id\n",
    "  and s1.start_date<=s2.end_date\n",
    "and s1.end_date>=s2.start_date\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c9e9682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+\n",
      "|user_id| join_date|favorite_brand|\n",
      "+-------+----------+--------------+\n",
      "|      1|2018-01-01|        Lenovo|\n",
      "|      2|2018-02-09|       Samsung|\n",
      "|      3|2018-01-19|            LG|\n",
      "|      4|2018-05-21|            HP|\n",
      "+-------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user=[('1', '2018-01-01', 'Lenovo'), ('2', '2018-02-09', 'Samsung'), \n",
    "('3', '2018-01-19', 'LG'), ('4', '2018-05-21', 'HP')]\n",
    "usercolumns=[\"user_id\",\"join_date\",\"favorite_brand\"]\n",
    "userdf=spark.createDataFrame(user,usercolumns)\n",
    "userdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aaa86347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-------+--------+---------+\n",
      "|order_id|order_date|item_id|buyer_id|seller_id|\n",
      "+--------+----------+-------+--------+---------+\n",
      "|       1|2019-08-01|      4|       1|        2|\n",
      "|       2|2018-08-02|      2|       1|        3|\n",
      "|       3|2019-08-03|      3|       2|        3|\n",
      "|       4|2018-08-04|      1|       4|        2|\n",
      "|       5|2018-08-04|      1|       3|        4|\n",
      "|       6|2019-08-05|      2|       2|        4|\n",
      "+--------+----------+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders1=[('1', '2019-08-01', '4', '1', '2'),\n",
    "('2', '2018-08-02', '2', '1', '3'),('3', '2019-08-03', '3', '2', '3'),\n",
    "('4', '2018-08-04', '1', '4', '2'),('5', '2018-08-04', '1', '3', '4'),\n",
    "('6', '2019-08-05', '2', '2', '4')]\n",
    "ordercolumns=[\"order_id\",\"order_date\",\"item_id\",\"buyer_id\",\"seller_id\"]\n",
    "orderdf=spark.createDataFrame(orders1,ordercolumns)\n",
    "orderdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a103a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|item_id|item_brand|\n",
      "+-------+----------+\n",
      "|      1|   Samsung|\n",
      "|      2|    Lenovo|\n",
      "|      3|        LG|\n",
      "|      4|        HP|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "items=[('1', 'Samsung'),('2', 'Lenovo'),('3', 'LG'),('4', 'HP')]\n",
    "itemcolumns=[\"item_id\",\"item_brand\"]\n",
    "itemdf=spark.createDataFrame(items,itemcolumns)\n",
    "itemdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8e5c2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "userdf.createOrReplaceTempView(\"users\")\n",
    "orderdf.createOrReplaceTempView(\"orders1\")\n",
    "itemdf.createOrReplaceTempView(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "be9e2a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n",
      "|buyer_id| join_date|orders_in_2019|\n",
      "+--------+----------+--------------+\n",
      "|       1|2018-01-01|             1|\n",
      "|       2|2018-02-09|             2|\n",
      "|       3|2018-01-19|             0|\n",
      "|       4|2018-05-21|             0|\n",
      "+--------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT     u.user_id AS buyer_id,\n",
    "           u.join_date,\n",
    "           IFNULL(c.order_count, 0) AS orders_in_2019\n",
    "FROM       Users u\n",
    "LEFT JOIN (SELECT   buyer_id,\n",
    "                    COUNT(buyer_id) AS order_count\n",
    "           FROM    (SELECT buyer_id\n",
    "                    FROM   Orders1\n",
    "                    WHERE  order_date >= '2019-01-01') o\n",
    "           GROUP BY buyer_id) c\n",
    "ON         u.user_id = c.buyer_id\n",
    "ORDER BY   u.user_id ASC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "96b23361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|revenue|month|\n",
      "+---+-------+-----+\n",
      "|  1|   8000|  Jan|\n",
      "|  2|   9000|  Jan|\n",
      "|  3|  10000|  Feb|\n",
      "|  1|   7000|  Feb|\n",
      "|  1|   6000|  Mar|\n",
      "|  2|   6000|  Mar|\n",
      "+---+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "department=[('1', '8000', 'Jan'),('2', '9000', 'Jan'),\n",
    "\t\t\t\t\t\t\t('3', '10000', 'Feb'),('1', '7000', 'Feb'),\n",
    "\t\t\t\t\t\t\t('1', '6000', 'Mar'),('2','6000','Mar')]\n",
    "departmentcolumns=[\"id\",\"revenue\",\"month\"]\n",
    "departmentdf=spark.createDataFrame(department,departmentcolumns)\n",
    "departmentdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cca6c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "departmentdf.createOrReplaceTempView(\"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3f3fe000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "| id|Jan_Revenue|Feb_Revenue|Mar_Revenue|Apr_Revenue|May_Revenue|Jun_Revenue|Jul_Revenue|Aug_Revenue|Sep_Revenue|Oct_Revenue|Nov_Revenue|Dec_Revenue|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "|  1|     8000.0|     7000.0|     6000.0|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  2|     9000.0|       null|     6000.0|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "|  3|       null|    10000.0|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n",
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT id, \n",
    "SUM(IF (month = \"Jan\", revenue, null)) AS Jan_Revenue, \n",
    "SUM(IF (month = \"Feb\", revenue, null)) AS Feb_Revenue, \n",
    "SUM(IF (month = \"Mar\", revenue, null)) AS Mar_Revenue, \n",
    "SUM(IF (month = \"Apr\", revenue, null)) AS Apr_Revenue,\n",
    " SUM(IF (month = \"May\", revenue, null)) AS May_Revenue, \n",
    " SUM(IF (month = \"Jun\", revenue, null)) AS Jun_Revenue,\n",
    " SUM(IF (month = \"Jul\", revenue, null)) AS Jul_Revenue, \n",
    " SUM(IF (month = \"Aug\", revenue, null)) AS Aug_Revenue, \n",
    " SUM(IF (month = \"Sep\", revenue, null)) AS Sep_Revenue,\n",
    " SUM(IF (month = \"Oct\", revenue, null)) AS Oct_Revenue, \n",
    " SUM(IF (month = \"Nov\", revenue, null)) AS Nov_Revenue, \n",
    " SUM(IF (month = \"Dec\", revenue, null)) AS Dec_Revenue \n",
    " FROM Department GROUP BY id\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "50eef589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+-----+\n",
      "|  stock_name|operation|operation_day|price|\n",
      "+------------+---------+-------------+-----+\n",
      "|      Maggie|      Buy|            1| 1000|\n",
      "|Corona Masks|      Buy|            2|   10|\n",
      "|      Maggie|     Sell|            5| 9000|\n",
      "|    Handbags|      Buy|           17|30000|\n",
      "|Corona Masks|     Sell|            3| 1010|\n",
      "|Corona Masks|      Buy|            4| 1000|\n",
      "|Corona Masks|     Sell|            5|  500|\n",
      "|Corona Masks|      Buy|            6| 1000|\n",
      "|    Handbags|     Sell|           29| 7000|\n",
      "|Corona Masks|     Sell|           10|10000|\n",
      "+------------+---------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "stock=[('Maggie', 'Buy', '1', '1000'),('Corona Masks', 'Buy', '2', '10'),\n",
    "('Maggie', 'Sell', '5', '9000'),('Handbags', 'Buy', '17', '30000'),\n",
    "('Corona Masks', 'Sell', '3', '1010'),('Corona Masks', 'Buy', '4', '1000'),\n",
    "('Corona Masks', 'Sell', '5', '500'),('Corona Masks', 'Buy', '6', '1000'),\n",
    "('Handbags', 'Sell', '29', '7000'),('Corona Masks', 'Sell', '10', '10000')]\n",
    "operation=Enum('operation',['Buy','sell'])\n",
    "stockcolumns=[\"stock_name\",\"operation\",\"operation_day\",\"price\"]\n",
    "stockdf=spark.createDataFrame(stock,stockcolumns)\n",
    "stockdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "924b250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockdf.createOrReplaceTempView(\"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1c14e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "|  stock_name|capital_gain_loss|\n",
      "+------------+-----------------+\n",
      "|      Maggie|           8000.0|\n",
      "|Corona Masks|           9500.0|\n",
      "|    Handbags|         -23000.0|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT stock_name, \n",
    "SUM( Case When operation='Buy' then -price When operation='Sell' then price End)\n",
    " As capital_gain_loss FROM Stocks Group By stock_name\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd60992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
