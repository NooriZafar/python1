#installing pyspark

--->  !pip install pyspark

#creating sparksssion

--->  import pyspark
	from pyspark.sql import *
	spark=SparkSession.builder.appName('practise').getOrCreate() ->sparksession is used to create dtaframe

#Reading file from folder and displaying data

--->  df_pyspark=spark.read.csv('text1.csv')
	df_pyspark.show()
output:

 +------+---+
|   _c0|_c1|
+------+---+
|  Name|Age|
|  Pavi| 21|
| Noori| 21|
|Joshna| 21|
|Sindhu| 22|
|Suhana| 22|
| Aliya| 22|
+------+---+

#Displaying schema and the file with header names

--->  df_pyspark=spark.read.csv('text1.csv',header=True,inferSchema=True)
	df_pyspark.printSchema()
output:

root
 |-- Name: string (nullable = true)
 |-- Age: integer (nullable = true)
	
	df_pyspark.show()
output:

+------+---+
|  Name|Age|
+------+---+
|  Pavi| 21|
| Noori| 21|
|Joshna| 21|
|Sindhu| 22|
|Suhana| 22|
| Aliya| 22|
+------+---+

#displaying all column names

--->  df_pyspark.columns

output:

['Name', 'Age']

#displaying only selected column data

--->  df_pyspark.select(['age','name']).show()

output:

+------+
|  Name|
+------+
|  Pavi| 
| Noori| 
|Joshna| 
|Sindhu|
|Suhana|
| Aliya| 
+------+

#displaying data types

--->  df_pyspark.dtypes

output:
[('Name', 'string'), ('Age', 'int')]

#describe the table

--->  df_pyspark.describe().show()

output:
+-------+------+------------------+
|summary|  Name|               Age|
+-------+------+------------------+
|  count|     5|                 5|
|   mean|  null|              21.4|
| stddev|  null|0.5477225575051661|
|    min|Joshna|                21|
|    max|Suhana|                22|
+-------+------+------------------+

#how to add new column

--->  df_pyspark.withColumn('age2',df_pyspark['age']+2).show()

output:
+------+---+----+
|  Name|Age|age2|
+------+---+----+
|  Pavi| 21|  23|
| Noori| 21|  23|
|Joshna| 21|  23|
|Sindhu| 22|  24|
|Suhana| 22|  24|
+------+---+----+

df_pyspark.withColumn('age after 2 years',df_pyspark['age']+2).show()
+------+---+-----------------+
|  Name|Age|age after 2 years|
+------+---+-----------------+
|  Pavi| 21|               23|
| Noori| 21|               23|
|Joshna| 21|               23|
|Sindhu| 22|               24|
|Suhana| 22|               24|
+------+---+-----------------+

#How to drop a column

--->  df_pyspark.drop('age after 2 years').show()

output:
+------+---+
|  Name|Age|
+------+---+
|  Pavi| 21|
| Noori| 21|
|Joshna| 21|
|Sindhu| 22|
|Suhana| 22|
+------+---+

#renaming a existing column

--->  df_pyspark.withColumnRenamed('Name','name').show()

output:
+------+---+
|  name|Age|
+------+---+
|  Pavi| 21|
| Noori| 21|
|Joshna| 21|
|Sindhu| 22|
|Suhana| 22|
+------+---+

#to display only selected number of rows

--->  df_pyspark.head(3)

output:
[Row(Name='Pavi', Age=21),
 Row(Name='Noori', Age=21),
 Row(Name='Joshna', Age=21)]

#how to perform add operation on two columns

--->  df_pyspark.withColumn('total',df_pyspark.Age + df_pyspark.Age).show()


output:
+------+---+-----+
|  Name|Age|total|
+------+---+-----+
|  Pavi| 21|   42|
| Noori| 21|   42|
|Joshna| 21|   42|
|Sindhu| 22|   44|
|Suhana| 22|   44|
+------+---+-----+


#creating table directly without uploading file

--->  simpleData = [("James","Sales","NY",90000,34,10000),
    ("Michael","Sales","NY",86000,56,20000),
    ("Robert","Sales","CA",81000,30,23000),
    ("Maria","Finance","CA",90000,24,23000),
    ("Raman","Finance","CA",99000,40,24000),
    ("Scott","Finance","NY",83000,36,19000),
    ("Jen","Finance","NY",79000,53,15000),
    ("Jeff","Marketing","CA",80000,25,18000),
    ("Kumar","Marketing","NY",91000,50,21000)
  ]

	schema = ["employee_name","department","state","salary","age","bonus"]
	df = spark.createDataFrame(data=simpleData, schema = schema)
	df.printSchema()
	df.show(truncate=False)

output:
root
 |-- employee_name: string (nullable = true)
 |-- department: string (nullable = true)
 |-- state: string (nullable = true)
 |-- salary: long (nullable = true)
 |-- age: long (nullable = true)
 |-- bonus: long (nullable = true)

+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|James        |Sales     |NY   |90000 |34 |10000|
|Michael      |Sales     |NY   |86000 |56 |20000|
|Robert       |Sales     |CA   |81000 |30 |23000|
|Maria        |Finance   |CA   |90000 |24 |23000|
|Raman        |Finance   |CA   |99000 |40 |24000|
|Scott        |Finance   |NY   |83000 |36 |19000|
|Jen          |Finance   |NY   |79000 |53 |15000|
|Jeff         |Marketing |CA   |80000 |25 |18000|
|Kumar        |Marketing |NY   |91000 |50 |21000|
+-------------+----------+-----+------+---+-----+


#performing groupby operation using different conditions

--->  df.groupBy("department").count().show()

output:
+----------+-----+
|department|count|
+----------+-----+
|     Sales|    3|
|   Finance|    4|
| Marketing|    2|
+----------+-----+

--->  df.groupBy("department","state").max().show()

output:
+----------+-----+-----------+--------+----------+
|department|state|max(salary)|max(age)|max(bonus)|
+----------+-----+-----------+--------+----------+
|     Sales|   CA|      81000|      30|     23000|
|   Finance|   CA|      99000|      40|     24000|
|     Sales|   NY|      90000|      56|     20000|
|   Finance|   NY|      83000|      53|     19000|
| Marketing|   NY|      91000|      50|     21000|
| Marketing|   CA|      80000|      25|     18000|
+----------+-----+-----------+--------+----------+

--->  df.groupBy("department","state").sum("salary","bonus").show()

output:
+----------+-----+-----------+----------+
|department|state|sum(salary)|sum(bonus)|
+----------+-----+-----------+----------+
|     Sales|   CA|      81000|     23000|
|   Finance|   CA|     189000|     47000|
|     Sales|   NY|     176000|     30000|
|   Finance|   NY|     162000|     34000|
| Marketing|   NY|      91000|     21000|
| Marketing|   CA|      80000|     18000|
+----------+-----+-----------+----------+

--->  df.groupBy("department").agg(sum("salary").alias("sum_salary"),avg("salary").alias("avg_salary")).show()

output:
+----------+----------+-----------------+
|department|sum_salary|       avg_salary|
+----------+----------+-----------------+
|     Sales|    257000|85666.66666666667|
|   Finance|    351000|          87750.0|
| Marketing|    171000|          85500.0|
+----------+----------+-----------------+

--->df.groupBy("department").agg(sum("salary").alias("sum_salary"),
                             avg("salary").alias("avg_salary"))\
                             .where(col("sum_salary")>200000).show()
output:using conditions

+----------+----------+-----------------+
|department|sum_salary|       avg_salary|
+----------+----------+-----------------+
|     Sales|    257000|85666.66666666667|
|   Finance|    351000|          87750.0|
+----------+----------+-----------------+


#performing different operation using sort()

--->  df.sort("department").show()

output:
+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        Raman|   Finance|   CA| 99000| 40|24000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|        Scott|   Finance|   NY| 83000| 36|19000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|        James|     Sales|   NY| 90000| 34|10000|
|      Michael|     Sales|   NY| 86000| 56|20000|
|       Robert|     Sales|   CA| 81000| 30|23000|
+-------------+----------+-----+------+---+-----+

--->  df.sort("department","state").show()

output:
+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        Raman|   Finance|   CA| 99000| 40|24000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|        Scott|   Finance|   NY| 83000| 36|19000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
|       Robert|     Sales|   CA| 81000| 30|23000|
|        James|     Sales|   NY| 90000| 34|10000|
|      Michael|     Sales|   NY| 86000| 56|20000|
+-------------+----------+-----+------+---+-----+


# ordering the data in required form

--->  df.orderBy(desc("employee_name")).show()

output:
+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|        Scott|   Finance|   NY| 83000| 36|19000|
|       Robert|     Sales|   CA| 81000| 30|23000|
|        Raman|   Finance|   CA| 99000| 40|24000|
|      Michael|     Sales|   NY| 86000| 56|20000|
|        Maria|   Finance|   CA| 90000| 24|23000|
|        Kumar| Marketing|   NY| 91000| 50|21000|
|          Jen|   Finance|   NY| 79000| 53|15000|
|         Jeff| Marketing|   CA| 80000| 25|18000|
|        James|     Sales|   NY| 90000| 34|10000|
+-------------+----------+-----+------+---+-----+

#to display whole table by using show()

--->  df_pyspark.show(truncate=False)

#to display only particular column values

--->  df_pyspark.show(truncate=23)

output:


#default it display only 20 rows

--->  df_pyspark.show()

#display particular rows and column values

--->  df.show(2,truncate=3)

output:
+-------------+----------+-----+------+---+-----+
|employee_name|department|state|salary|age|bonus|
+-------------+----------+-----+------+---+-----+
|          Jam|       Sal|   NY|   900| 34|  100|
|          Mic|       Sal|   NY|   860| 56|  200|
+-------------+----------+-----+------+---+-----+
only showing top 2 rows


#performing operation using substring()

--->  data = [["1","20200828"],["2","20180525"]]
	columns=["id","date"]
	df=spark.createDataFrame(data,columns)
	df.show()

output:
+---+--------+
| id|    date|
+---+--------+
|  1|20200828|
|  2|20180525|
+---+--------+


# by using substring we can seperate date,month,year

--->  df1=df.select('date',substring('date',1,4).alias("year"),substring('date',5,7).alias("month"),substring('date',7,9).alias("day"))
	df1.show()

output:

+--------+----+-----+---+
|    date|year|month|day|
+--------+----+-----+---+
|20200828|2020| 0828| 28|
|20180525|2018| 0525| 25|
+--------+----+-----+---+


#create a table using stucttype and stuctfield

--->  data = [
    (("James","","Smith"),["Java","Scala","C++"],"OH","M"),
    (("Anna","Rose",""),["Spark","Java","C++"],"NY","F"),
    (("Julia","","Williams"),["CSharp","VB"],"OH","F"),
    (("Maria","Anne","Jones"),["CSharp","VB"],"NY","M"),
    (("Jen","Mary","Brown"),["CSharp","VB"],"NY","M"),
    (("Mike","Mary","Williams"),["Python","VB"],"OH","M")
 ]

	schema = StructType([
     StructField('name', StructType([
        StructField('firstname', StringType(), True),
        StructField('middlename', StringType(), True),
         StructField('lastname', StringType(), True)
     ])),
     StructField('languages', ArrayType(StringType()), True),
     StructField('state', StringType(), True),
     StructField('gender', StringType(), True)
 ])
	
	df = spark.createDataFrame(data = data, schema = schema)
	df.printSchema()
	df.show(truncate=False)

output:

root
 |-- name: struct (nullable = true)
 |    |-- firstname: string (nullable = true)
 |    |-- middlename: string (nullable = true)
 |    |-- lastname: string (nullable = true)
 |-- languages: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- state: string (nullable = true)
 |-- gender: string (nullable = true)

+----------------------+------------------+-----+------+
|name                  |languages         |state|gender|
+----------------------+------------------+-----+------+
|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |
|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |
|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |
|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |
|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |
|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |
+----------------------+------------------+-----+------+

#performing filter operation on above table

--->  df.filter(df.state == "NY").show(truncate=False)

output:
+--------------------+------------------+-----+------+
|name                |languages         |state|gender|
+--------------------+------------------+-----+------+
|{Anna, Rose, }      |[Spark, Java, C++]|NY   |F     |
|{Maria, Anne, Jones}|[CSharp, VB]      |NY   |M     |
|{Jen, Mary, Brown}  |[CSharp, VB]      |NY   |M     |
+--------------------+------------------+-----+------+

#using filter display gender is F

--->  df.filter("gender == 'F'").show()

output:
+-------------------+------------------+-----+------+
|               name|         languages|state|gender|
+-------------------+------------------+-----+------+
|     {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|
|{Julia, , Williams}|      [CSharp, VB]|   OH|     F|
+-------------------+------------------+-----+------+

#selecting multiple columns using filter

--->  df.filter((df.state == "OH")& (df.gender == "F")).show(truncate=False)

output:

+-------------------+------------+-----+------+
|name               |languages   |state|gender|
+-------------------+------------+-----+------+
|{Julia, , Williams}|[CSharp, VB]|OH   |F     |
+-------------------+------------+-----+------+

#to select distict values

--->  df.distinct()
























































































